{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crescendonow/thai_geoparsing/blob/main/IJG_3_lstm_60epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64mf6JbnaTsj"
      },
      "source": [
        "learning_rate = default,\\n\n",
        "drop_out = 0.3,\\n\n",
        "epochs = 50,\\n\n",
        "activation = 'softmax',\\n\n",
        "optimizer = 'adam',\\n\n",
        "unit = 64,\\n\n",
        "dense = n_tag(37)\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aSKmA-Uqrj6"
      },
      "outputs": [],
      "source": [
        "#Install every library for use in notebook\n",
        "!pip install pickle-mixin\n",
        "!pip install tqdm\n",
        "!pip install pythainlp[full]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "metadata": {
        "id": "mhwUT0YGSJTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py==2.10.0' --force-reinstall"
      ],
      "metadata": {
        "id": "fJML7xYyUpuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensor2tensor \n",
        "!pip install t2t-trainer "
      ],
      "metadata": {
        "id": "yfvceRApTwu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C2WfALIUIpz",
        "outputId": "e393f9c2-8e31-4f1c-8af8-a579152a106d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: Loaded: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ_6iPkT2zOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4801083-0382-4583-f5b0-be48533a9a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-p4k_56ce\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-p4k_56ce\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.9.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=b8d86f76a989700d6879872c64879f4a07a1a049d3774c654981d30725131f55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9fk6z8a7/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phlKQJZ_YQnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891660ca-05f6-4666-872b-b8c590c06c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "ab591da3-2309-4490-a3d7-499bbe9de9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 31 09:19:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#check GPU runtime\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMj1jISLpzTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d17957a-bbfe-4faf-c81b-c1828597d5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "#check ram usage\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Lr9vVmnAq4"
      },
      "outputs": [],
      "source": [
        "#If you want to use bi-lstm+crf ; set tf 1.xx\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr5gkcy71yiQ",
        "outputId": "02e3c508-105c-4a4b-ba1f-3644d2275547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "# Save / Load File\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "# Plot Graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn Report\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from itertools import chain\n",
        "\n",
        "# Load Vectors\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Utility\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Model Utility\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Keras Model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Input\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
        "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6mUmojo3ZDj"
      },
      "outputs": [],
      "source": [
        "#load train and test data\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/1.Tokennization/train_dn.data', 'rb') as token:\n",
        "    train_data = pickle.load(token)\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/1.Tokennization/validate_dn.data', 'rb') as token2:\n",
        "    validate_data = pickle.load(token2)   \n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/1.Tokennization/test_attacut_cl.data', 'rb') as token3:\n",
        "    test_data = pickle.load(token3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load dict ner_dict, chardict\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/5.dict/ner_ix.dict', 'rb') as ner:\n",
        "    ner_to_ix = pickle.load(ner)\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/5.dict/chardict.pickle', 'rb') as chardict:\n",
        "    char2idx = pickle.load(chardict)"
      ],
      "metadata": {
        "id": "VlGGa4jqHLyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dict word index, thai2fit dict and thai2fit index\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/5.dict/word_index.dict', 'rb') as word_index:\n",
        "    word_to_ix = pickle.load(word_index)\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/5.dict/thai2index.dict', 'rb') as thai_index:\n",
        "    thai2dict_to_ix = pickle.load(thai_index)\n",
        "with open('/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/5.dict/thai2dict.dict', 'rb') as thai_to_dict:\n",
        "    thai2dict = pickle.load(thai_to_dict)   "
      ],
      "metadata": {
        "id": "-dmp7nYkH2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b9EgDNb3ZW0"
      },
      "outputs": [],
      "source": [
        "thai2fit_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/dissertation/thai2vecNoSym.bin',binary=True)\n",
        "thai2fit_weight = thai2fit_model.vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
        "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
        "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
        "\n",
        "n_word = len(word_to_ix)\n",
        "n_tag = len(ner_to_ix)\n",
        "n_thai2dict = len(thai2dict_to_ix)\n",
        "n_chars = len(char2idx)\n",
        "\n",
        "print(n_word)\n",
        "print(n_tag)\n",
        "print(n_thai2dict)\n",
        "print(ner_to_ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXZZoD6dIT2i",
        "outputId": "47306d6b-d6cf-46a0-f17d-32f22eaed7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41375\n",
            "38\n",
            "55677\n",
            "{'O': 0, 'B-ACP': 1, 'I-ACP': 2, 'B-ADMIN': 3, 'I-ADMIN': 4, 'B-BSN': 5, 'I-BSN': 6, 'B-DEP': 7, 'I-DEP': 8, 'B-FPLACE': 9, 'I-FPLACE': 10, 'B-GOV': 11, 'I-GOV': 12, 'B-HP': 13, 'I-HP': 14, 'B-MKT': 15, 'I-MKT': 16, 'B-MON': 17, 'I-MON': 18, 'B-NAT': 19, 'I-NAT': 20, 'B-OTHER': 21, 'I-OTHER': 22, 'B-RCT': 23, 'I-RCT': 24, 'B-RES': 25, 'I-RES': 26, 'B-ROAD': 27, 'I-ROAD': 28, 'B-RP': 29, 'I-RP': 30, 'B-RT': 31, 'I-RT': 32, 'B-STORE': 33, 'I-STORE': 34, 'B-TRAN': 35, 'I-TRAN': 36, 'pad': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Parameter and Hyper Parameter\n",
        "max_len = 250\n",
        "max_len_char = 30\n",
        "\n",
        "character_LSTM_unit = 32\n",
        "char_embedding_dim = 32\n",
        "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
        "lstm_recurrent_dropout = 0.5\n",
        "\n",
        "train_batch_size = 32\n",
        "train_epochs = 60"
      ],
      "metadata": {
        "id": "KDJC6nONIT5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlhG1FKh3ZdM"
      },
      "outputs": [],
      "source": [
        "#mapping function if unkwnon from embedding use \"unknown\"\n",
        "def prepare_sequence_word(input_text):\n",
        "    idxs = list()\n",
        "    for word in input_text:\n",
        "        if word in thai2dict:\n",
        "            idxs.append(thai2dict_to_ix[word])\n",
        "        else:\n",
        "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
        "    return idxs\n",
        "\n",
        "def prepare_sequence_target(input_label):\n",
        "    idxs = [ner_to_ix[w] for w in input_label]\n",
        "    return idxs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent =[ [ word[0] for word in sent]for sent in train_data ] #words only\n",
        "train_targets =[ [ word[2] for word in sent]for sent in train_data ] #NER only\n",
        "\n",
        "input_val_sent =[ [ word[0] for word in sent]for sent in validate_data ] #words only\n",
        "validate_targets =[ [ word[2] for word in sent]for sent in validate_data ] #NER only\n",
        "\n",
        "input_test_sent =[ [ word[0] for word in sent]for sent in test_data ] #words only\n",
        "test_targets =[ [ word[2] for word in sent]for sent in test_data ] #NER only"
      ],
      "metadata": {
        "id": "538PqxNMJTHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare Training Dataset\n",
        "\n",
        "## Word Training\n",
        "X_word_tr = [prepare_sequence_word(s) for s in input_sent]\n",
        "X_word_tr = pad_sequences(maxlen=max_len, sequences=X_word_tr, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "\n",
        "## Character Training\n",
        "X_char_tr = []\n",
        "for sentence in train_data:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len):\n",
        "        word_seq = []\n",
        "        for j in range(max_len_char):\n",
        "            try:\n",
        "                if(sentence[i][0][j] in char2idx):\n",
        "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
        "                else:\n",
        "                    word_seq.append(char2idx.get(\"unknown\"))\n",
        "            except:\n",
        "                word_seq.append(char2idx.get(\"pad\"))\n",
        "        sent_seq.append(word_seq)\n",
        "    X_char_tr.append(np.array(sent_seq))\n",
        "\n",
        "## Sequence Label Training\n",
        "y_tr = [prepare_sequence_target(s) for s in train_targets]\n",
        "y_tr = pad_sequences(maxlen=max_len, sequences=y_tr, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "y_tr = [to_categorical(i, num_classes=n_tag) for i in y_tr]"
      ],
      "metadata": {
        "id": "gW82UiuhJTKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare Validate Dataset\n",
        "## Word validattion\n",
        "X_word_va = [prepare_sequence_word(s) for s in input_val_sent]\n",
        "X_word_va = pad_sequences(maxlen=max_len, sequences=X_word_va, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "\n",
        "## Character Training\n",
        "X_char_va = []\n",
        "for sentence in validate_data:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len):\n",
        "        word_seq = []\n",
        "        for j in range(max_len_char):\n",
        "            try:\n",
        "                if(sentence[i][0][j] in char2idx):\n",
        "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
        "                else:\n",
        "                    word_seq.append(char2idx.get(\"unknown\"))\n",
        "            except:\n",
        "                word_seq.append(char2idx.get(\"pad\"))\n",
        "        sent_seq.append(word_seq)\n",
        "    X_char_va.append(np.array(sent_seq))\n",
        "\n",
        "## Sequence Label Training\n",
        "y_va = [prepare_sequence_target(s) for s in validate_targets]\n",
        "y_va = pad_sequences(maxlen=max_len, sequences=y_va, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "y_va = [to_categorical(i, num_classes=n_tag) for i in y_va]"
      ],
      "metadata": {
        "id": "dNrNqwIjJTNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare Testing Dataset\n",
        "\n",
        "## Word Testing\n",
        "X_word_te = [prepare_sequence_word(s) for s in input_test_sent]\n",
        "X_word_te = pad_sequences(maxlen=max_len, sequences=X_word_te, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "\n",
        "## Character Testing\n",
        "X_char_te = []\n",
        "for sentence in test_data:\n",
        "    sent_seq = []\n",
        "    for i in range(max_len):\n",
        "        word_seq = []\n",
        "        for j in range(max_len_char):\n",
        "            try:\n",
        "                if(sentence[i][0][j] in char2idx):\n",
        "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
        "                else:\n",
        "                    word_seq.append(char2idx.get(\"unknown\"))\n",
        "            except:\n",
        "                word_seq.append(char2idx.get(\"pad\"))    \n",
        "        sent_seq.append(word_seq)\n",
        "    X_char_te.append(np.array(sent_seq))\n",
        "\n",
        "## Sequence Label Testing\n",
        "y_te = [prepare_sequence_target(s) for s in test_targets]\n",
        "y_te = pad_sequences(maxlen=max_len, sequences=y_te, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
        "y_te = [to_categorical(i, num_classes=n_tag) for i in y_te]"
      ],
      "metadata": {
        "id": "A1kin2x-JzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = f'/content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/'"
      ],
      "metadata": {
        "id": "xSKtcWLXJ-HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Keras Model\n",
        "# Word Input\n",
        "word_in = Input(shape=(max_len,), name='word_input_')\n",
        "\n",
        "# Word Embedding Using Thai2Fit\n",
        "word_embeddings = Embedding(input_dim=n_thai2dict,\n",
        "                            output_dim=400,\n",
        "                            weights = [thai2fit_weight],input_length=max_len,\n",
        "                            mask_zero=False,\n",
        "                            name='word_embedding', trainable=False)(word_in)\n",
        "\n",
        "# Character Input\n",
        "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
        "\n",
        "# Character Embedding\n",
        "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
        "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
        "\n",
        "# Character Sequence to Vector via BiLSTM\n",
        "char_enc = TimeDistributed(LSTM(units=character_LSTM_unit, return_sequences=False, recurrent_dropout=lstm_recurrent_dropout))(emb_char)\n",
        "\n",
        "\n",
        "# Concatenate All Embedding\n",
        "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
        "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
        "\n",
        "# Main Model BiLSTM\n",
        "main_lstm = LSTM(units=main_lstm_unit, return_sequences=True,\n",
        "                               recurrent_dropout=lstm_recurrent_dropout)(all_word_embeddings)\n",
        "out = TimeDistributed(Dense(n_tag, activation = 'softmax'))(main_lstm)\n",
        "\n",
        "#Model\n",
        "model = Model([word_in, char_in], out)\n",
        "\n",
        "#load save model and continue train model from last epochs\n",
        "#load_filepath=MODEL_PATH+\"last_weights60.hdf5\"\n",
        "#model.load_weights(load_filepath)\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvEmiab6KAxd",
        "outputId": "7e6e4c1f-a98d-45e4-ccc1-8612c92d67f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "char_input (InputLayer)         (None, 250, 30)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_input_ (InputLayer)        (None, 250)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 250, 30, 32)  12768       char_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 250, 400)     22270800    word_input_[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 250, 32)      8320        time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 250, 432)     0           word_embedding[0][0]             \n",
            "                                                                 time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 250, 432)     0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 250, 256)     705536      spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 250, 38)      9766        lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 23,007,190\n",
            "Trainable params: 736,390\n",
            "Non-trainable params: 22,270,800\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=MODEL_PATH+\"weights-improvement-{epoch:02d}-{accuracy:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit([X_word_tr,\n",
        "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))\n",
        "                     ],\n",
        "                     np.array(y_tr),\n",
        "                     batch_size=train_batch_size, epochs=train_epochs, verbose=1,callbacks=callbacks_list,\n",
        "                     validation_data=(\n",
        "                     [X_word_va,\n",
        "                     np.array(X_char_va).reshape((len(X_char_va), max_len, max_len_char))\n",
        "                     ],\n",
        "                     np.array(y_va))\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh_Ts9VcKA5u",
        "outputId": "299c79d9-d2bb-45ea-e5c6-9497bcc25c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 17955 samples, validate on 4489 samples\n",
            "Epoch 1/60\n",
            "17955/17955 [==============================] - 149s 8ms/step - loss: 0.0154 - accuracy: 0.9974 - val_loss: 0.0139 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00001: accuracy improved from -inf to 0.99742, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-01-0.997.hdf5\n",
            "Epoch 2/60\n",
            "17955/17955 [==============================] - 149s 8ms/step - loss: 0.0140 - accuracy: 0.9975 - val_loss: 0.0125 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00002: accuracy improved from 0.99742 to 0.99746, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-02-0.997.hdf5\n",
            "Epoch 3/60\n",
            "17955/17955 [==============================] - 147s 8ms/step - loss: 0.0131 - accuracy: 0.9975 - val_loss: 0.0115 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00003: accuracy improved from 0.99746 to 0.99748, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-03-0.997.hdf5\n",
            "Epoch 4/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.0110 - val_accuracy: 0.9977\n",
            "\n",
            "Epoch 00004: accuracy improved from 0.99748 to 0.99753, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-04-0.998.hdf5\n",
            "Epoch 5/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0120 - accuracy: 0.9975 - val_loss: 0.0104 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00005: accuracy improved from 0.99753 to 0.99754, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-05-0.998.hdf5\n",
            "Epoch 6/60\n",
            "17955/17955 [==============================] - 148s 8ms/step - loss: 0.0115 - accuracy: 0.9976 - val_loss: 0.0102 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00006: accuracy improved from 0.99754 to 0.99758, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-06-0.998.hdf5\n",
            "Epoch 7/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.0100 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00007: accuracy improved from 0.99758 to 0.99761, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-07-0.998.hdf5\n",
            "Epoch 8/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.0099 - val_accuracy: 0.9978\n",
            "\n",
            "Epoch 00008: accuracy improved from 0.99761 to 0.99766, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-08-0.998.hdf5\n",
            "Epoch 9/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.0100 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00009: accuracy improved from 0.99766 to 0.99769, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-09-0.998.hdf5\n",
            "Epoch 10/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.0094 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00010: accuracy improved from 0.99769 to 0.99771, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-10-0.998.hdf5\n",
            "Epoch 11/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0099 - accuracy: 0.9977 - val_loss: 0.0093 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00011: accuracy improved from 0.99771 to 0.99773, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-11-0.998.hdf5\n",
            "Epoch 12/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.0091 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00012: accuracy improved from 0.99773 to 0.99778, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-12-0.998.hdf5\n",
            "Epoch 13/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.0092 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00013: accuracy improved from 0.99778 to 0.99783, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-13-0.998.hdf5\n",
            "Epoch 14/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00014: accuracy improved from 0.99783 to 0.99786, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-14-0.998.hdf5\n",
            "Epoch 15/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00015: accuracy improved from 0.99786 to 0.99789, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-15-0.998.hdf5\n",
            "Epoch 16/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00016: accuracy improved from 0.99789 to 0.99793, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-16-0.998.hdf5\n",
            "Epoch 17/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.0088 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00017: accuracy improved from 0.99793 to 0.99798, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-17-0.998.hdf5\n",
            "Epoch 18/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0086 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00018: accuracy did not improve from 0.99798\n",
            "Epoch 19/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.0088 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00019: accuracy improved from 0.99798 to 0.99804, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-19-0.998.hdf5\n",
            "Epoch 20/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0086 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00020: accuracy improved from 0.99804 to 0.99806, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-20-0.998.hdf5\n",
            "Epoch 21/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0087 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00021: accuracy improved from 0.99806 to 0.99808, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-21-0.998.hdf5\n",
            "Epoch 22/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0087 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00022: accuracy improved from 0.99808 to 0.99813, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-22-0.998.hdf5\n",
            "Epoch 23/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0084 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00023: accuracy improved from 0.99813 to 0.99816, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-23-0.998.hdf5\n",
            "Epoch 24/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0084 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00024: accuracy improved from 0.99816 to 0.99816, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-24-0.998.hdf5\n",
            "Epoch 25/60\n",
            "17955/17955 [==============================] - 149s 8ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0083 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00025: accuracy improved from 0.99816 to 0.99821, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-25-0.998.hdf5\n",
            "Epoch 26/60\n",
            "17955/17955 [==============================] - 149s 8ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0084 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00026: accuracy improved from 0.99821 to 0.99826, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-26-0.998.hdf5\n",
            "Epoch 27/60\n",
            "17955/17955 [==============================] - 157s 9ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00027: accuracy improved from 0.99826 to 0.99827, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-27-0.998.hdf5\n",
            "Epoch 28/60\n",
            "17955/17955 [==============================] - 170s 9ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0082 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00028: accuracy improved from 0.99827 to 0.99830, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-28-0.998.hdf5\n",
            "Epoch 29/60\n",
            "17955/17955 [==============================] - 147s 8ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00029: accuracy improved from 0.99830 to 0.99832, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-29-0.998.hdf5\n",
            "Epoch 30/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.0083 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00030: accuracy improved from 0.99832 to 0.99835, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-30-0.998.hdf5\n",
            "Epoch 31/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0084 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00031: accuracy improved from 0.99835 to 0.99839, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-31-0.998.hdf5\n",
            "Epoch 32/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00032: accuracy improved from 0.99839 to 0.99839, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-32-0.998.hdf5\n",
            "Epoch 33/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0084 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00033: accuracy improved from 0.99839 to 0.99844, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-33-0.998.hdf5\n",
            "Epoch 34/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00034: accuracy improved from 0.99844 to 0.99844, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-34-0.998.hdf5\n",
            "Epoch 35/60\n",
            "17955/17955 [==============================] - 147s 8ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0085 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00035: accuracy improved from 0.99844 to 0.99847, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-35-0.998.hdf5\n",
            "Epoch 36/60\n",
            "17955/17955 [==============================] - 147s 8ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00036: accuracy improved from 0.99847 to 0.99849, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-36-0.998.hdf5\n",
            "Epoch 37/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00037: accuracy improved from 0.99849 to 0.99851, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-37-0.999.hdf5\n",
            "Epoch 38/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0086 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00038: accuracy did not improve from 0.99851\n",
            "Epoch 39/60\n",
            "17955/17955 [==============================] - 150s 8ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0084 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00039: accuracy improved from 0.99851 to 0.99856, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-39-0.999.hdf5\n",
            "Epoch 40/60\n",
            "17955/17955 [==============================] - 148s 8ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00040: accuracy improved from 0.99856 to 0.99859, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-40-0.999.hdf5\n",
            "Epoch 41/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00041: accuracy did not improve from 0.99859\n",
            "Epoch 42/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00042: accuracy improved from 0.99859 to 0.99860, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-42-0.999.hdf5\n",
            "Epoch 43/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00043: accuracy improved from 0.99860 to 0.99864, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-43-0.999.hdf5\n",
            "Epoch 44/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00044: accuracy did not improve from 0.99864\n",
            "Epoch 45/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00045: accuracy did not improve from 0.99864\n",
            "Epoch 46/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00046: accuracy improved from 0.99864 to 0.99868, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-46-0.999.hdf5\n",
            "Epoch 47/60\n",
            "17955/17955 [==============================] - 144s 8ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00047: accuracy improved from 0.99868 to 0.99868, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-47-0.999.hdf5\n",
            "Epoch 48/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9980\n",
            "\n",
            "Epoch 00048: accuracy improved from 0.99868 to 0.99869, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-48-0.999.hdf5\n",
            "Epoch 49/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00049: accuracy improved from 0.99869 to 0.99871, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-49-0.999.hdf5\n",
            "Epoch 50/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00050: accuracy improved from 0.99871 to 0.99873, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-50-0.999.hdf5\n",
            "Epoch 51/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0086 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00051: accuracy did not improve from 0.99873\n",
            "Epoch 52/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00052: accuracy improved from 0.99873 to 0.99877, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-52-0.999.hdf5\n",
            "Epoch 53/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0087 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00053: accuracy did not improve from 0.99877\n",
            "Epoch 54/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0089 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00054: accuracy improved from 0.99877 to 0.99877, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-54-0.999.hdf5\n",
            "Epoch 55/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0087 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00055: accuracy improved from 0.99877 to 0.99877, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-55-0.999.hdf5\n",
            "Epoch 56/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00056: accuracy improved from 0.99877 to 0.99879, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-56-0.999.hdf5\n",
            "Epoch 57/60\n",
            "17955/17955 [==============================] - 147s 8ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0089 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00057: accuracy improved from 0.99879 to 0.99880, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-57-0.999.hdf5\n",
            "Epoch 58/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00058: accuracy improved from 0.99880 to 0.99881, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-58-0.999.hdf5\n",
            "Epoch 59/60\n",
            "17955/17955 [==============================] - 145s 8ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00059: accuracy improved from 0.99881 to 0.99883, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-59-0.999.hdf5\n",
            "Epoch 60/60\n",
            "17955/17955 [==============================] - 146s 8ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0088 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00060: accuracy improved from 0.99883 to 0.99885, saving model to /content/drive/MyDrive/dissertation/TEXT DATA/TrainingData/Model_train/weights-improvement-60-0.999.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_filepath=MODEL_PATH+\"lstm_last_weight-60.hdf5\"\n",
        "model.save_weights(save_filepath)"
      ],
      "metadata": {
        "id": "VlZg9Yibemd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M88MkUn1EVE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c787394-abbb-4e36-d412-cc890d8800ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5617/5617 [==============================] - 18s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "pred_model = model.predict([X_word_te,np.array(X_char_te).reshape((len(X_char_te),max_len, max_len_char))], verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for i in range(0,len(pred_model)):\n",
        "    try:\n",
        "        out = np.argmax(pred_model[i], axis=-1)\n",
        "        true = np.argmax(y_te[i], axis=-1)\n",
        "        revert_pred=[ix_to_ner[i] for i in out]\n",
        "        revert_true=[ix_to_ner[i] for i in true]\n",
        "        y_pred.append(revert_pred)\n",
        "        y_true.append(revert_true)\n",
        "    except:\n",
        "        print (i)"
      ],
      "metadata": {
        "id": "X1Vsjfs4x6RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_classification_report(y_true, y_pred):\n",
        " \n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "    tagset = set(lb.classes_) - {'O', 'pad'} #{'pad'} \n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    #tagset = list(sorted(set(lb.classes_)))\n",
        "    #tagset = tagset[:-2]\n",
        "    print(tagset)\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "        digits=4\n",
        "    )"
      ],
      "metadata": {
        "id": "ApRiVaknx6UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_classification_report(y_true,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPu5OaRgx6Wy",
        "outputId": "cb94702a-1076-4125-ef7d-e4da9ecdbcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B-ACP', 'I-ACP', 'B-ADMIN', 'I-ADMIN', 'B-BSN', 'I-BSN', 'B-DEP', 'I-DEP', 'B-FPLACE', 'I-FPLACE', 'B-GOV', 'I-GOV', 'B-HP', 'I-HP', 'B-MKT', 'I-MKT', 'B-MON', 'I-MON', 'B-NAT', 'I-NAT', 'B-OTHER', 'I-OTHER', 'B-RCT', 'I-RCT', 'B-RES', 'I-RES', 'B-ROAD', 'I-ROAD', 'B-RP', 'I-RP', 'B-RT', 'I-RT', 'B-STORE', 'I-STORE', 'B-TRAN', 'I-TRAN']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ACP     0.5278    0.5135    0.5205        37\n",
            "       I-ACP     0.7979    0.7143    0.7538       105\n",
            "     B-ADMIN     0.7848    0.7670    0.7758       309\n",
            "     I-ADMIN     0.9444    0.7612    0.8430        67\n",
            "       B-BSN     0.7857    0.4231    0.5500        26\n",
            "       I-BSN     0.8966    0.6190    0.7324        84\n",
            "       B-DEP     0.7805    0.6214    0.6919       103\n",
            "       I-DEP     0.7018    0.7143    0.7080       168\n",
            "    B-FPLACE     0.7564    0.5960    0.6667        99\n",
            "    I-FPLACE     0.9091    0.7407    0.8163        27\n",
            "       B-GOV     0.7778    0.5600    0.6512        25\n",
            "       I-GOV     0.7033    0.7033    0.7033        91\n",
            "        B-HP     0.9286    0.5417    0.6842        24\n",
            "        I-HP     0.9615    0.5952    0.7353        84\n",
            "       B-MKT     0.7333    0.6111    0.6667        18\n",
            "       I-MKT     0.7778    1.0000    0.8750        14\n",
            "       B-MON     1.0000    0.5000    0.6667         2\n",
            "       I-MON     0.0000    0.0000    0.0000         1\n",
            "       B-NAT     1.0000    0.4545    0.6250        11\n",
            "       I-NAT     0.8750    0.5833    0.7000        12\n",
            "     B-OTHER     1.0000    0.3333    0.5000         3\n",
            "     I-OTHER     1.0000    1.0000    1.0000         6\n",
            "       B-RCT     0.7027    0.5909    0.6420        44\n",
            "       I-RCT     0.7375    0.6941    0.7152        85\n",
            "       B-RES     0.6800    0.5667    0.6182        30\n",
            "       I-RES     0.6771    0.7303    0.7027        89\n",
            "      B-ROAD     0.6867    0.6645    0.6754       155\n",
            "      I-ROAD     0.7248    0.7714    0.7474       140\n",
            "        B-RP     0.8958    0.8431    0.8687        51\n",
            "        I-RP     0.6794    0.8241    0.7448       108\n",
            "        B-RT     0.7054    0.7054    0.7054       129\n",
            "        I-RT     0.6737    0.8394    0.7475       492\n",
            "     B-STORE     0.5312    0.7083    0.6071        24\n",
            "     I-STORE     0.7188    0.7931    0.7541        87\n",
            "      B-TRAN     0.7000    0.5385    0.6087        39\n",
            "      I-TRAN     0.8077    0.5943    0.6848       106\n",
            "\n",
            "   micro avg     0.7332    0.7178    0.7254      2895\n",
            "   macro avg     0.7656    0.6449    0.6858      2895\n",
            "weighted avg     0.7453    0.7178    0.7236      2895\n",
            " samples avg     0.0015    0.0015    0.0015      2895\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def  chunk_extract(tag_list,sentence_id):\n",
        "  \"\"\"\n",
        "  >>> chunk_extract(['O', 'B-PER', 'I-PER', 'B-ORG', 'O'], 1)\n",
        "  [(1, 1, 3, 'PER'), (1, 3, 4, 'ORG')]\n",
        "  \"\"\"\n",
        "  nes = []\n",
        "  for i, tag in enumerate(tag_list):\n",
        "    if tag[0] == 'B':\n",
        "        ner_type = tag[2:]\n",
        "        if i < len(tag_list)-1:  \n",
        "            current = i + 1\n",
        "            if tag_list[current] == 'O':\n",
        "                nes.append((sentence_id,i,i, ner_type)) \n",
        "            elif tag_list[current] == 'I-{}'.format(ner_type):    \n",
        "                while tag_list[current] == 'I-{}'.format(ner_type) and current < (len(tag_list)-1):\n",
        "                    current += 1\n",
        "                nes.append((sentence_id,i, current, ner_type))\n",
        "        else:\n",
        "            nes.append((sentence_id,i,i, ner_type))\n",
        "  return nes"
      ],
      "metadata": {
        "id": "8Q1cOc2px6Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_id = [x for x in range(len(y_te))]"
      ],
      "metadata": {
        "id": "cb-cokk0x6cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_test = set()\n",
        "for sent,idx in tqdm(zip(y_true,sentence_id)):\n",
        "    ch = chunk_extract(sent,idx)\n",
        "    chunk_test.update(ch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AG_rRbqx6e_",
        "outputId": "77f0dcb5-62fa-43aa-93bc-f3c10efa489d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5617it [00:00, 43652.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_pred = set()\n",
        "for sent,idx in tqdm(zip(y_pred,sentence_id)):\n",
        "    ch = chunk_extract(sent,idx)\n",
        "    chunk_pred.update(ch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JuhXqYFx6h2",
        "outputId": "c39d26c4-dd11-4a1f-8607-b452e9097cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5617it [00:00, 26144.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_phrase(true,prediction):\n",
        "    total_correct = len(true.intersection(prediction))\n",
        "    total_predict = len(prediction)\n",
        "    total_true = len(true)\n",
        "    \n",
        "    precision = total_correct/total_predict\n",
        "    recall = total_correct/total_true\n",
        "    f1 = (2 * precision * recall)/(precision + recall)\n",
        "    \n",
        "    print('total_correct:',total_correct,':','total_predict:',total_predict,':','total_true:',total_true)\n",
        "    print('precision:', round(precision,3))\n",
        "    print('recall:', round(recall,3))\n",
        "    print('f1:', round(f1,3))\n",
        "    return [round(precision,3),round(recall,3),round(f1,3)]"
      ],
      "metadata": {
        "id": "-3mbpq8lzvTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_phrase(chunk_test,chunk_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rvyEZqBz1hD",
        "outputId": "517be244-0b82-4166-e6bb-bbe5bf666d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_correct: 558 : total_predict: 916 : total_true: 1083\n",
            "precision: 0.609\n",
            "recall: 0.515\n",
            "f1: 0.558\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.609, 0.515, 0.558]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_test = list(chunk_test)\n",
        "chunk_pred = list(chunk_pred)"
      ],
      "metadata": {
        "id": "vLs8rIIGz1kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_phrase_type(chunk_test,chunk_pred):\n",
        "    total_pred, total_true, total_correct = 0, 0, 0\n",
        "    df = pd.DataFrame(columns=['PRECISION','RECALL','F1','SUPPORT'])\n",
        "    ent_types = ['ACP','ADMIN','BSN','DEP','FPLACE','GOV','HP','MKT','MON','NAT','OTHER','RCT','RES','ROAD','RP','RT','STORE','TRAN']\n",
        "    list_correct = []\n",
        "    for ent in ent_types:\n",
        "        true_set = []\n",
        "        pred_set = []\n",
        "        type_pred,type_true,type_correct = 0,0,0\n",
        "        for tag in chunk_test:\n",
        "            if tag[3] == ent:\n",
        "                true_set.append(tag)\n",
        "            if ent in tag:\n",
        "                type_true+=1\n",
        "                tag_true = set(true_set)\n",
        "        for tag2 in chunk_pred:\n",
        "            if tag2[3] == ent:\n",
        "                pred_set.append(tag2)\n",
        "            if ent in tag2:\n",
        "                type_pred+=1\n",
        "                tag_pred = set(pred_set)\n",
        "        type_correct=len(tag_true.intersection(tag_pred))\n",
        "        try:\n",
        "            precision = type_correct/type_pred\n",
        "        except:\n",
        "            precision = 0\n",
        "        recall = type_correct/type_true\n",
        "        try:\n",
        "            f1 = (2 * precision * recall) / (precision + recall)\n",
        "        except:\n",
        "            f1 = 0\n",
        "        list_correct.append([type_true,type_pred,type_correct,ent])\n",
        "        df.loc[ent] = [round(precision,3), round(recall,3), round(f1,3), str(type_true)]\n",
        "\n",
        "    #Calculate micro macro f1\n",
        "    total_true,total_pred,total_correct = 0,0,0\n",
        "    for p in list_correct:\n",
        "        total_true+=p[0]\n",
        "        total_pred+=p[1]\n",
        "        total_correct+=p[2]\n",
        "    precision_micro = total_correct / total_pred\n",
        "    recall_micro = total_correct / total_true\n",
        "    f1_micro = (2 * precision_micro * recall_micro) / (precision_micro + recall_micro)\n",
        "    df.loc['MACRO'] = [round(df.PRECISION.mean(),3), round(df.RECALL.mean(),3), round(df.F1.mean(),3), str(total_true)]\n",
        "    df.loc['MICRO'] = [round(precision_micro,3), round(recall_micro,3), round(f1_micro,3), str(total_true)]\n",
        "\n",
        "    print(df)"
      ],
      "metadata": {
        "id": "Z9C2Tj2rz1m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_phrase_type(chunk_test,chunk_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSW0Ojswz1pv",
        "outputId": "feeecec1-6d16-472e-c77a-e222b658bf16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        PRECISION  RECALL     F1 SUPPORT\n",
            "ACP         0.552   0.444  0.492      36\n",
            "ADMIN       0.766   0.727  0.746     289\n",
            "BSN         0.615   0.320  0.421      25\n",
            "DEP         0.527   0.390  0.448     100\n",
            "FPLACE      0.757   0.596  0.667      94\n",
            "GOV         0.556   0.400  0.465      25\n",
            "HP          0.769   0.417  0.541      24\n",
            "MKT         0.533   0.444  0.485      18\n",
            "MON         1.000   0.500  0.667       2\n",
            "NAT         0.750   0.273  0.400      11\n",
            "OTHER       1.000   0.500  0.667       2\n",
            "RCT         0.576   0.432  0.494      44\n",
            "RES         0.455   0.333  0.385      30\n",
            "ROAD        0.603   0.566  0.584     145\n",
            "RP          0.400   0.375  0.387      48\n",
            "RT          0.353   0.318  0.335     129\n",
            "STORE       0.318   0.304  0.311      23\n",
            "TRAN        0.731   0.500  0.594      38\n",
            "MACRO       0.626   0.435  0.505    1083\n",
            "MICRO       0.609   0.515  0.558    1083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'h5py==2.10.0' --force-reinstall"
      ],
      "metadata": {
        "id": "NQjXDPYQ1HWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}